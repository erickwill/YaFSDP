diff --git a/src/accelerate/accelerator.py b/src/accelerate/accelerator.py
index fb60473..046a04a 100755
--- a/src/accelerate/accelerator.py
+++ b/src/accelerate/accelerator.py
@@ -31,6 +31,7 @@ from typing import Any, Callable, Union
 
 import torch
 import torch.utils.hooks as hooks
+from ya_fsdp import YaFSDP
 
 from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state
 from .data_loader import DataLoaderDispatcher, prepare_data_loader, skip_first_batches
@@ -1363,42 +1364,59 @@ class Accelerator:
                 # don't wrap it again
                 # In case the model is already compiled using PyTorch 2.0 and the wrapped model in it
                 # is a FSDP model, don't wrap it again
-                is_type_fsdp = isinstance(model, FSDP) or (
+                is_type_fsdp = isinstance(model, YaFSDP) or isinstance(model, FSDP) or (
                     is_compiled_module(model) and isinstance(model._orig_mod, FSDP)
                 )
 
                 if not is_type_fsdp:
-                    self.state.fsdp_plugin.set_auto_wrap_policy(model)
                     fsdp_plugin = self.state.fsdp_plugin
-                    kwargs = {
-                        "sharding_strategy": fsdp_plugin.sharding_strategy,
-                        "cpu_offload": fsdp_plugin.cpu_offload,
-                        "auto_wrap_policy": fsdp_plugin.auto_wrap_policy,
-                        "mixed_precision": fsdp_plugin.mixed_precision_policy,
-                        "sync_module_states": fsdp_plugin.sync_module_states,
-                        "backward_prefetch": fsdp_plugin.backward_prefetch,
-                        "forward_prefetch": fsdp_plugin.forward_prefetch,
-                        "use_orig_params": fsdp_plugin.use_orig_params,
-                        "param_init_fn": fsdp_plugin.param_init_fn,
-                        "ignored_modules": fsdp_plugin.ignored_modules,
-                        "limit_all_gathers": fsdp_plugin.limit_all_gathers,
-                        "device_id": self.device,
-                    }
-                    model = FSDP(model, **kwargs)
+                    self.state.fsdp_plugin.set_auto_wrap_policy(model)
+                    if fsdp_plugin.ya_fsdp_enabled:
+                        assert fsdp_plugin.ya_fsdp_modules_to_wrap_with_names is not None
+                        model = YaFSDP(
+                            model,
+                            zero_stage=3,
+                            param_dtype=fsdp_plugin.mixed_precision_policy.param_dtype,
+                            modules_to_wrap_with_names=fsdp_plugin.ya_fsdp_modules_to_wrap_with_names,
+                            gradient_accumulation_steps=self.gradient_accumulation_steps,
+                            rogue_layer_norm_modules_with_names=fsdp_plugin.ya_fsdp_rogue_layer_norm_modules_with_names,
+                            sync_module_states=fsdp_plugin.sync_module_states,
+                            param_init_fn=fsdp_plugin.param_init_fn,
+                            layer_norm_module_cls=fsdp_plugin.ya_fsdp_layer_norm_module_cls,
+                            device_id=self.device,
+                        )
+                    else:
+                        kwargs = {
+                            "sharding_strategy": fsdp_plugin.sharding_strategy,
+                            "cpu_offload": fsdp_plugin.cpu_offload,
+                            "auto_wrap_policy": fsdp_plugin.auto_wrap_policy,
+                            "mixed_precision": fsdp_plugin.mixed_precision_policy,
+                            "sync_module_states": fsdp_plugin.sync_module_states,
+                            "backward_prefetch": fsdp_plugin.backward_prefetch,
+                            "forward_prefetch": fsdp_plugin.forward_prefetch,
+                            "use_orig_params": fsdp_plugin.use_orig_params,
+                            "param_init_fn": fsdp_plugin.param_init_fn,
+                            "ignored_modules": fsdp_plugin.ignored_modules,
+                            "limit_all_gathers": fsdp_plugin.limit_all_gathers,
+                            "device_id": self.device,
+                        }
+                        model = FSDP(model, **kwargs)
                     if fsdp_plugin.activation_checkpointing:
                         from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import (
                             CheckpointImpl,
                             apply_activation_checkpointing,
                             checkpoint_wrapper,
                         )
-
+                        auto_wrap_policy = fsdp_plugin.activation_checkpointing_auto_wrap_policy
+                        if auto_wrap_policy is None:
+                            auto_wrap_policy = fsdp_plugin.auto_wrap_policy
                         apply_activation_checkpointing(
                             model,
                             checkpoint_wrapper_fn=functools.partial(
                                 checkpoint_wrapper,
                                 checkpoint_impl=CheckpointImpl.NO_REENTRANT,
                             ),
-                            auto_wrap_policy=fsdp_plugin.auto_wrap_policy,
+                            auto_wrap_policy=auto_wrap_policy,
                         )
                 # if the previous and current models are same, delete the previous one
                 if len(self._models) > 1 and (self._models[-2] is self._models[-1]):
@@ -3061,7 +3079,9 @@ class Accelerator:
             from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
 
             full_state_dict_config = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
-            with FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, full_state_dict_config):
+            with contextlib.ExitStack() as cm_stack:
+                if not self.state.fsdp_plugin.ya_fsdp_enabled:
+                    cm_stack.enter_context(FSDP.state_dict_type(model, StateDictType.FULL_STATE_DICT, full_state_dict_config))
                 state_dict = model.state_dict()
         else:
             if unwrap:
diff --git a/src/accelerate/commands/launch.py b/src/accelerate/commands/launch.py
index e315f9d..b04e5dd 100644
--- a/src/accelerate/commands/launch.py
+++ b/src/accelerate/commands/launch.py
@@ -546,6 +546,16 @@ def launch_command_parser(subparsers=None):
         help="If True, each individually wrapped FSDP unit will broadcast module parameters from rank 0."
         " (useful only when `use_fsdp` flag is passed).",
     )
+    fsdp_args.add_argument(
+        "--fsdp_ya_fsdp_enabled",
+        default="false",
+        type=str,
+    )
+    fsdp_args.add_argument(
+        "--fsdp_num_layers_to_checkpoint",
+        default=None,
+        type=int,
+    )
 
     # megatron_lm args
     megatron_lm_args = parser.add_argument_group("Megatron-LM Arguments", "Arguments related to Megatron-LM.")
diff --git a/src/accelerate/utils/dataclasses.py b/src/accelerate/utils/dataclasses.py
index 730c777..3e9906a 100644
--- a/src/accelerate/utils/dataclasses.py
+++ b/src/accelerate/utils/dataclasses.py
@@ -26,7 +26,7 @@ import warnings
 from contextlib import contextmanager
 from dataclasses import dataclass, field
 from datetime import timedelta
-from typing import Any, Callable, Dict, Iterable, List, Literal, Optional, Tuple, get_args
+from typing import Any, Callable, Dict, Iterable, List, Literal, Optional, Tuple, get_args, Type
 
 import torch
 
@@ -976,6 +976,12 @@ class FullyShardedDataParallelPlugin:
             "for reduced memory usage."
         },
     )
+    activation_checkpointing_auto_wrap_policy: Optional[Callable] = field(default=None)
+    ya_fsdp_enabled: bool = field(default=False)
+    ya_fsdp_modules_to_wrap_with_names: Optional[list[tuple[torch.nn.Module, str]]] = field(default=None)
+    ya_fsdp_rogue_layer_norm_modules_with_names: Optional[dict[torch.nn.Module, str]] = field(default=None)
+    ya_fsdp_layer_norm_module_cls: Optional[Type[torch.nn.Module]] = field(default=None)
+    num_layers_to_checkpoint: Optional[int] = field(default=None)
 
     def __post_init__(self):
         from torch.distributed.fsdp.fully_sharded_data_parallel import BackwardPrefetch, CPUOffload, ShardingStrategy
@@ -1008,6 +1014,12 @@ class FullyShardedDataParallelPlugin:
         self.sync_module_states = str_to_bool(os.environ.get(prefix + "SYNC_MODULE_STATES", "True")) == 1
         self.forward_prefetch = str_to_bool(os.environ.get(prefix + "FORWARD_PREFETCH", "False")) == 1
         self.activation_checkpointing = str_to_bool(os.environ.get(prefix + "ACTIVATION_CHECKPOINTING", "False")) == 1
+        self.ya_fsdp_enabled = str_to_bool(os.environ.get(prefix + "YA_FSDP_ENABLED", "False")) == 1
+        self.num_layers_to_checkpoint = (
+            int(os.environ.get(prefix + "NUM_LAYERS_TO_CHECKPOINT"))
+            if prefix + "NUM_LAYERS_TO_CHECKPOINT" in os.environ
+            else None
+        )
 
         if self.sync_module_states:
             if is_npu_available():
@@ -1073,6 +1085,12 @@ class FullyShardedDataParallelPlugin:
                     self.auto_wrap_policy = functools.partial(
                         size_based_auto_wrap_policy, min_num_params=min_num_params
                     )
+        else:
+            self.auto_wrap_policy = self.auto_wrap_policy(model)
+            self.activation_checkpointing_auto_wrap_policy = self.activation_checkpointing_auto_wrap_policy(model)
+            if self.ya_fsdp_enabled:
+                self.ya_fsdp_modules_to_wrap_with_names = self.ya_fsdp_modules_to_wrap_with_names(model)
+                self.ya_fsdp_rogue_layer_norm_modules_with_names = self.ya_fsdp_rogue_layer_norm_modules_with_names(model)
 
     def set_mixed_precision(self, mixed_precision):
         if mixed_precision == "fp16":
diff --git a/src/accelerate/utils/launch.py b/src/accelerate/utils/launch.py
index e748e37..2d7c64d 100644
--- a/src/accelerate/utils/launch.py
+++ b/src/accelerate/utils/launch.py
@@ -203,6 +203,10 @@ def prepare_multi_gpu_env(args: argparse.Namespace) -> Dict[str, str]:
         current_env["FSDP_USE_ORIG_PARAMS"] = str(args.fsdp_use_orig_params).lower()
         current_env["FSDP_CPU_RAM_EFFICIENT_LOADING"] = str(args.fsdp_cpu_ram_efficient_loading).lower()
         current_env["FSDP_SYNC_MODULE_STATES"] = str(args.fsdp_sync_module_states).lower()
+        current_env["FSDP_ACTIVATION_CHECKPOINTING"] = str(args.fsdp_activation_checkpointing).lower()
+        current_env["FSDP_YA_FSDP_ENABLED"] = str(args.fsdp_ya_fsdp_enabled).lower()
+        if args.fsdp_num_layers_to_checkpoint is not None:
+            current_env["FSDP_NUM_LAYERS_TO_CHECKPOINT"] = str(args.fsdp_num_layers_to_checkpoint).lower()
 
     if args.use_megatron_lm:
         prefix = "MEGATRON_LM_"
